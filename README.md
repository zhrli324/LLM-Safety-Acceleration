# LLM-Safety-Acceleration

A repo for our new research: **Modular Residual Risk Detection and Inference Acceleration System for Large Language Models**

Since the application of the Large Language Models, there has been an endless stream of research on the interpretability of model jailbreaking. Researchers are eager to know what secrets are hidden in the "black box" of the neural network layer of the jailbreaked large model. This project preposes a framework for analyzing the security risks of large model residual flows using a low burden algorithm design, which can identify malicious requests in the early stages of model inference and accelerate inference. In addition, we also provide pluggable defense and acceleration models for multiple open-source language models.